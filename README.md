# ğŸ•‰ï¸ GitaGPT â€” Advanced HyDE-Based RAG for Bhagavad Gita Q&A

> A Context-Grounded Retrieval-Augmented Generation (RAG) API + Web UI for Exploring the Teachings of the Bhagavad Gita

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-Backend-green)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-red)
![ChromaDB](https://img.shields.io/badge/VectorDB-ChromaDB-purple)
![Groq](https://img.shields.io/badge/LLM-Llama--3.3--70B-orange)
![Embeddings](https://img.shields.io/badge/Embeddings-all--MiniLM--L6--v2-blue)

---

# ğŸ“˜ Overview

GitaGPT is an advanced **Retrieval-Augmented Generation (RAG)** system designed to answer Bhagavad Gitaâ€“related questions using structured Q&A knowledge and semantic retrieval.

It includes:

- âš¡ FastAPI backend API
- ğŸ’» Streamlit web interface (`app.py`)
- ğŸ§  HyDE (Hypothetical Document Expansion)
- ğŸ” ChromaDB semantic vector search
- ğŸ“Š Similarity score filtering
- ğŸ”’ Strict context-grounded answering
- ğŸ§¬ Deterministic embeddings (SHA256 IDs)

The system is optimized for:

- Conceptual understanding
- Explanation-based learning
- Reduced hallucination
- Scalable scripture upgrades

---

# ğŸš€ Key Features

## ğŸ§  1. HyDE Retrieval Enhancement

Before querying the vector database, the system generates a **hypothetical Gita-style explanation** using an LLM.

Pipeline:

```
User Question â†’ HyDE Expansion â†’ Enhanced Query
```

This improves retrieval for abstract spiritual queries like:

- What is karma yoga?
- What is detachment?
- What is true duty?
- How to overcome attachment?

---

## ğŸ” 2. Vector Search with Similarity Filtering

Uses:

- `sentence-transformers/all-MiniLM-L6-v2`
- Normalized embeddings
- Cosine similarity
- Configurable similarity threshold

Weak matches are filtered before generation, reducing hallucination risk.

---

## ğŸ”’ 3. Strict Context-Constrained Answering

The LLM is instructed to:

- Answer ONLY using retrieved context
- Refuse when insufficient data
- Avoid external knowledge
- Limit responses to 5â€“6 lines

This ensures grounded output.

---

## ğŸ§¬ 4. Deterministic Embedding Architecture

- SHA256-based vector IDs
- Duplicate prevention
- Batch-based ingestion
- Persistent Chroma storage
- Apple Silicon GPU (MPS) support

---

# ğŸ— Architecture Overview

```
User Question
     â†“
HyDE Expansion (LLM)
     â†“
Enhanced Query = Question + HyDE
     â†“
Vector Search (ChromaDB)
     â†“
Similarity Threshold Filtering
     â†“
Context-Constrained LLM Response
     â†“
Answer + Sources Returned
```

---

# ğŸ“‚ Project Structure

```
rag-project-dev/
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ ingest/
â”‚   â”œâ”€â”€ download_dataset.py
â”‚   â”œâ”€â”€ clean_dataset.py
â”‚   â”œâ”€â”€ chunk_dataset.py
â”‚   â””â”€â”€ embed_chunks.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw.json
â”‚   â”œâ”€â”€ cleaned.json
â”‚   â””â”€â”€ chunk_dataset.json
â”‚
â”œâ”€â”€ chroma_db/
â”‚
â”œâ”€â”€ app.py                # Streamlit Web UI
â”‚
â”œâ”€â”€ setup.sh              # Creates venv & installs dependencies
â”œâ”€â”€ activate.sh           # Activates virtual environment
â”œâ”€â”€ deactivate.sh         # Deactivates virtual environment
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# ğŸ“¦ Chunk Format

Structured Q&A chunks are stored in `chunk_dataset.json` as:

```json
{
  "text": "Dhritarashtra is blind, both physically and symbolically â€” representing ignorance...",
  "metadata": {
    "doc_id": 0,
    "source_question": "Why does Dhritarashtra ask Sanjaya to describe the battlefield?",
    "source_file": "Bhagavad-Gita-QA",
    "chunk_index": 0,
    "total_chunks": 1,
    "type": "qa_explanation"
  }
}
```

Design goals:

- Question-aware retrieval
- Explanation-focused learning
- Clean metadata tracking
- Future upgrade path toward verse-level ingestion

---

# âš™ï¸ Environment Setup

## ğŸ§ macOS / Linux (Recommended)

### 1ï¸âƒ£ Run Automated Setup

```bash
bash setup.sh
```

This will:

- Create `venv/`
- Activate environment
- Upgrade pip
- Install dependencies

### â–¶ï¸ Activate Environment

```bash
source activate.sh
```

### â¹ï¸ Deactivate Environment

```bash
source deactivate.sh
```

---

## ğŸªŸ Windows Setup (Manual)

### 1ï¸âƒ£ Create Virtual Environment

```powershell
python -m venv venv
```

### 2ï¸âƒ£ Activate

PowerShell:

```powershell
venv\Scripts\Activate.ps1
```

CMD:

```cmd
venv\Scripts\activate.bat
```

### 3ï¸âƒ£ Install Dependencies

```powershell
python -m pip install --upgrade pip
pip install -r requirements.txt
```

### â¹ï¸ Deactivate

```powershell
deactivate
```

---

# ğŸ“Š Data Ingestion Pipeline

## Step 1 â€” Download Dataset

```bash
python ingest/download_dataset.py
```

## Step 2 â€” Clean Dataset

```bash
python ingest/clean_dataset.py
```

Transforms raw Q&A into structured JSON.

## Step 3 â€” Chunk Dataset

```bash
python ingest/chunk_dataset.py
```

Features:

- Adds doc_id and chunk metadata
- Avoids over-splitting short answers
- SHA256-based deduplication

## Step 4 â€” Embed into Chroma

```bash
python ingest/embed_chunks.py
```

Features:

- Normalized vectors
- Deterministic IDs
- Batch insertion
- Persistent storage in `chroma_db/`

---

# ğŸ–¥ Running the Application

## 1ï¸âƒ£ Start Backend API

```bash
uvicorn api.main:app --reload
```

API runs at:

```
http://127.0.0.1:8000
```

Interactive docs:

```
http://127.0.0.1:8000/docs
```

---

## 2ï¸âƒ£ Start Web UI

```bash
streamlit run app.py
```

UI opens at:

```
http://localhost:8501
```

---

# ğŸ“¡ API Usage

### POST `/ask`

### Request

```json
{
  "question": "What is the meaning of karma yoga?"
}
```

### Response

```json
{
  "answer": "...",
  "hyde_expansion": "...",
  "sources": [
    "Original source question text"
  ]
}
```

---

# ğŸ”’ Hallucination Minimization Strategy

| Technique | Implemented |
|------------|-------------|
| Temperature = 0 | âœ… |
| HyDE expansion | âœ… |
| Similarity threshold filtering | âœ… |
| Context-only answering | âœ… |
| Refusal behavior | âœ… |
| Deterministic embeddings | âœ… |

---

# ğŸ§  Current System Type

This system currently operates as:

> **Retrieval-Augmented FAQ Assistant for Bhagavad Gita Teachings**

It does NOT yet use verse-level scripture ingestion.

---

# ğŸš€ Planned Upgrade Path

- Ingest full Sanskrit + English translation
- Add chapter/verse metadata
- Enable precise scripture citation
- Hybrid BM25 + Vector retrieval
- Cross-encoder reranking
- Retrieval confidence scoring
- Self-verification LLM pass
- Async FastAPI optimization
- Docker deployment support

---

# ğŸ“ˆ System Maturity

This implementation includes:

- Multi-step retrieval reasoning
- HyDE-based semantic expansion
- Deterministic embedding architecture
- Structured ingestion pipeline
- Guarded LLM prompting
- GPU-ready embedding support
- API + UI interface

For an MVP devotional AI assistant, this is a robust and scalable foundation.

---

# ğŸ‘¨â€ğŸ’» Author

**Hrithik Umbarji**

Built with discipline, devotion, and a passion for spiritual learning.

---

# ğŸ•‰ï¸ Guiding Principle

â€œna hi jÃ±Änena sadá¹›Å›aá¹ pavitram iha vidyate.â€  
*There is nothing as purifying as true knowledge.* â€” Bhagavad Gita 4.38

---

â­ If this project supports your journey of learning and reflection, consider starring the repository.
